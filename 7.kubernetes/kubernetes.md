# Kubernetes

**Cluster**
![alt text](attachments/cluster.jpeg)
- Cluster = lâ€™intero datacenter Kubernetes
- Cosâ€™Ã¨ il cluster EKS:  composto da Control plane + Worker nodes

*Control plane:* API server, scheduler, controller manager, etcd. In EKS Ã¨ gestito da AWS. Non câ€™Ã¨ un tuo â€œmaster nodeâ€ su EC2.

*Worker nodes:* le istanze dove girano i Pod. Sono EC2 (providerID AWS), pronte, v1.33.4-eks-â€¦

- Comando utile: kubectl get nodes -o wide

**Namespace**
- Namespace = â€œstanzeâ€ dentro quel datacenter. PiÃ¹ namespace convivono nello stesso cluster.

Lâ€™isolamento Ã¨ solo logico: per isolare rete serve NetworkPolicy.

Quote/limiti si applicano a livello di namespace, non di cluster.

Cosâ€™Ã¨ un namespace

Partizione logica dentro un singolo cluster.

Contiene risorse â€œnamespacedâ€ (Pod, Service, Deployment, ConfigMap, Secret, â€¦).

Non contiene risorse â€œcluster-scopedâ€ (Node, PersistentVolume, ClusterRole, CRD, â€¦).

Serve per isolamento logico, quote/limiti, RBAC e naming.

Comandi tipici:

kubectl get ns

kubectl get pods -n <ns>

kubectl get all -n <ns>

**Deployment**
![alt text](attachments/deployaml.jpeg)

**Service**
![alt text](attachments/serviceyaml.jpeg)


**Worker Node**

in EKS un worker node tipicamente corrisponde a una istanza EC2.
- Ogni EC2 registrata al cluster = 1 nodo Kubernetes.
- I nodi stanno in node group (ASG o managed node groups).
- Alternative: con Fargate non gestisci EC2; i Pod girano su infrastruttura gestita e non amministri nodi.

Note utili:
- Un nodo puÃ² eseguire piÃ¹ Pod. Il limite dipende da vCPU/RAM e dai limiti Pod-per-nodo dellâ€™istanza.
- Control plane EKS Ã¨ separato e gestito da AWS, non Ã¨ nelle tue EC2.

 



 



**Kubectl**




**multi-tenancy**
Condivisione di risorse: Invece che avere un'infrastruttura separata per ogni cliente, diversi tenant utilizzano la stessa istanza software, lo stesso sistema operativo e la stessa infrastruttura hardware sottostante





# ** ğŸ§  CPU vs Memoria in Kubernetes â€” spiegato facile **
Tipo risorsa	A cosa serve	Se sbagli cosa succede
CPU	Quante istruzioni puÃ² eseguire il container nel tempo â†’ calcolo	Se Ã¨ troppo poca: il pod viene rallentato (throttling), ma NON crasha
Memoria	Quanta RAM puÃ² usare per dati in uso, heap, cache ecc.	Se Ã¨ troppa poca: il pod va in crash (OOMKilled)

ğŸ§  Trucco per ricordare (con analogia semplice):
Immagina un cuoco (container) che cucina in una cucina (pod):

ğŸ”¹ CPU = mani del cuoco

PiÃ¹ mani ha â†’ piÃ¹ puÃ² lavorare in parallelo. Se ha solo 1 mano â†’ lavora lentamente (throttling), ma non muore.

ğŸ”¹ Memoria = tavolo da lavoro (RAM)

Se il tavolo Ã¨ troppo piccolo e non ci sta la roba â†’ il piatto cade per terra â†’ crash (OOMKilled).



CONSUMI RISORSE KUBE

Verifica richieste/limiti dichiarati nei Pod

kubectl get pod -n prov-ippv2-svts-platform-namespace -o=custom-columns='NAME:.metadata.name,CPU_REQUEST:.spec.containers[*].resources.requests.cpu,CPU_LIMIT:.spec.containers[*].resources.limits.cpu,MEM_REQUEST:.spec.containers[*].resources.requests.memory,MEM_LIMIT:.spec.containers[*].resources.limits.memory'



Oppure un esempio piÃ¹ leggibile (usa kubectl describe):

kubectl describe pod <POD_NAME> -n <NAMESPACE>






**kubectl top pod -n prov-ippv2-svts-platform-namespace**

ğŸ§ª A cosa servono requests e limits?
Campo	        Significato	                                Esempio
requests.cpu	Quanta CPU viene prenotata per il container	Garantisce che il container abbia almeno questa quantitÃ 
limits.cpu	    Quanta CPU puÃ² usare al massimo	            Se la supera â†’ throttling
requests.memory	Quanta RAM prenotata	                    Il scheduler cerca un nodo con questa RAM
limits.memory	Quanta RAM puÃ² usare al massimo	            Se la supera â†’ OOMKilled ğŸ’¥