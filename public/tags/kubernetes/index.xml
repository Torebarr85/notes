<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Kubernetes on Notes</title>
    <link>http://localhost:1313/notes/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on Notes</description>
    <generator>Hugo -- 0.151.0</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/notes/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kubernetes: Touning resources</title>
      <link>http://localhost:1313/notes/kubernetes/kubernetes-touning-resources/</link>
      <pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/notes/kubernetes/kubernetes-touning-resources/</guid>
      <description>&lt;p&gt;Ok, andiamo dritti. Hai cambiato i values, i pod vecchi sono stati killati e i nuovi non partono. Facciamo troubleshooting in 6 check veloci. Copia–incolla e leggiamo i messaggi chiave.&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;Stato rollout (capire cosa manca)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;kubectl rollout status deploy/prov-ippv2-pr-platform-ippv2-emailsender -n prov-ippv2-pr-platform-namespace
kubectl get deploy/prov-ippv2-pr-platform-ippv2-emailsender -n prov-ippv2-pr-platform-namespace -o wide&lt;/p&gt;
&lt;h1 id=&#34;requests--what-the-scheduler-needs-to-place-a-pod&#34;&gt;Requests = what the scheduler needs to place a pod&lt;/h1&gt;
&lt;p&gt;cpu_request: 200m
memory_request: 512Mi&lt;/p&gt;
&lt;h1 id=&#34;limits&#34;&gt;Limits&lt;/h1&gt;
&lt;p&gt;memory_limit: 2048Mi     # keep this to avoid OOM&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kubernetes: &lt;titolo&gt;</title>
      <link>http://localhost:1313/notes/kubernetes/kubernetes-titolo/</link>
      <pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/notes/kubernetes/kubernetes-titolo/</guid>
      <description>&lt;h1 id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Cluster&lt;/strong&gt;
&lt;img alt=&#34;alt text&#34; loading=&#34;lazy&#34; src=&#34;./attachments/cluster.jpeg&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cluster = l’intero datacenter Kubernetes&lt;/li&gt;
&lt;li&gt;Cos’è il cluster EKS:  composto da Control plane + Worker nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Control plane:&lt;/em&gt; API server, scheduler, controller manager, etcd. In EKS è gestito da AWS. Non c’è un tuo “master node” su EC2.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Worker nodes:&lt;/em&gt; le istanze dove girano i Pod. Sono EC2 (providerID AWS), pronte, v1.33.4-eks-…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Comando utile: kubectl get nodes -o wide&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Namespace&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Namespace = “stanze” dentro quel datacenter. Più namespace convivono nello stesso cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;L’isolamento è solo logico: per isolare rete serve NetworkPolicy.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
